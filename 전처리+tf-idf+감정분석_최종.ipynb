{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd30247-e2a2-4601-92a9-f7e56df60de7",
   "metadata": {},
   "source": [
    "# 전처리, 키워드 추출, 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a23a37-e01b-4fa3-88fc-f0330a0d1659",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263fc8a-be6c-45b8-8d97-1c225a47011e",
   "metadata": {},
   "source": [
    "### 1) Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d920ab2-d671-43b5-8a65-6b273d8f2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08fda4-069c-4025-9418-02e72bbf973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e7694-bdff-49ee-8549-a59b079778e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150900c-5854-46d1-8b3c-5510956b7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/BITAminNLP /대구관광지리뷰.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b3d75-303e-4893-b5f4-6d41286f9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83918c64-6c99-4e68-8b18-215ffe757d12",
   "metadata": {},
   "source": [
    "### 2) Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c48bc9-f323-4422-9e06-86995877191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "# 정제 및 정규화 함수 정의\n",
    "def clean_and_normalize(text):\n",
    "    # 특수 문자와 숫자 제거\n",
    "    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', text)\n",
    "\n",
    "    # 중복되는 자음/모음 제거 (예: 'ㅋㅋㅋㅋ' -> 'ㅋㅋ')\n",
    "    text = repeat_normalize(text, num_repeats=2)\n",
    "\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# '리뷰' 컬럼에 대해 전처리 적용\n",
    "df['리뷰'] = df['리뷰'].apply(clean_and_normalize)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d93fd-6821-4b7b-9cbd-4bfdd7a7bba0",
   "metadata": {},
   "source": [
    "### 3) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f2f48-8c91-44db-93f1-a1cb9d889295",
   "metadata": {},
   "source": [
    "### STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2d306-e438-4dc5-98bc-3d7087d1b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = '/content/drive/MyDrive/BITAminNLP /stopword.txt'\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1867f6-c15c-49e6-ab1e-1c61b6565903",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stopwords, 'r', encoding='utf-8') as f:\n",
    "    stopword = set(f.read().split())\n",
    "\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d58ee-4413-446f-91d5-fca5404c3d1f",
   "metadata": {},
   "source": [
    "### 형태소 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee44500-8495-4fd1-85e2-d494e4e66f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "df['리뷰_형태소토큰화_Okt'] = df['리뷰'].apply(okt.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4becaa4e-1b41-4ec6-a252-ebcc0fa3696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수 정의 (토큰화된 데이터용)\n",
    "def remove_stopwords(tokenized_text):\n",
    "    meaningful_words = [word for word in tokenized_text if word not in stopword]  # 불용어가 아닌 단어만 남김\n",
    "    return meaningful_words  # 리스트 형태로 반환\n",
    "\n",
    "# 데이터프레임의 토큰화된 리뷰 컬럼에 대해 불용어 제거 적용\n",
    "df['리뷰_불용어제거'] = df['리뷰_형태소토큰화_Okt'].apply(remove_stopwords)\n",
    "\n",
    "# 결과 확인\n",
    "df[['리뷰_형태소토큰화_Okt', '리뷰_불용어제거']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ae04b-8124-480a-b020-133c812a0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('형태소토큰화.csv', index=False, encoding='utf-8-sig')\n",
    "df = df.drop('리뷰_형태소토큰화_Okt',axis=1)\n",
    "df = df.drop('리뷰_불용어제거',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0682e-80fb-4de6-8a3a-4ba4180cf521",
   "metadata": {},
   "source": [
    "### Subword 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfceb7-37b4-4be4-b0a0-97ed02f4314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34c8eb-2cb5-4ff6-ba77-f8fef5d05c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 모든 리뷰 텍스트를 하나의 파일로 저장\n",
    "with open('/content/data.txt', 'w', encoding='utf-8') as f:\n",
    "    for review in df['리뷰']:\n",
    "        f.write(review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29044159-0ea6-46f8-a5cf-f5fb2256a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# SentencePiece 모델 학습\n",
    "spm.SentencePieceTrainer.Train('--input=/content/data.txt --model_prefix=spm --vocab_size=32000 --model_type=bpe')\n",
    "\n",
    "# 모델 파일이 spm.model로 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34947882-118d-4cc7-9a4b-6a856df62ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('/content/spm.model')\n",
    "\n",
    "# 예시: 첫 번째 리뷰를 토큰화\n",
    "tokenized_review = sp.encode_as_pieces(df['리뷰'][0])\n",
    "print(tokenized_review)\n",
    "\n",
    "# 데이터프레임에 모든 리뷰 토큰화 결과 저장\n",
    "df['리뷰_Sentence_Piece'] = df['리뷰'].apply(lambda x: sp.encode_as_pieces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda4c64-659f-41c8-af80-7a21f030d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc858f6-e017-4985-a382-8b0206d224dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수 정의 (토큰화된 데이터용)\n",
    "def remove_stopwords(tokenized_text):\n",
    "    meaningful_words = [word for word in tokenized_text if word not in stopword]  # 불용어가 아닌 단어만 남김\n",
    "    return meaningful_words  # 리스트 형태로 반환\n",
    "\n",
    "# 데이터프레임의 토큰화된 리뷰 컬럼에 대해 불용어 제거 적용\n",
    "df['리뷰_불용어제거'] = df['리뷰_Sentence_Piece'].apply(remove_stopwords)\n",
    "\n",
    "# 결과 확인\n",
    "df[['리뷰_Sentence_Piece', '리뷰_불용어제거']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8751f-c026-482b-84e3-1cd704b0571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 데이터를 CSV로 저장\n",
    "df.to_csv('subword 토큰화.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1eb530-d510-4877-9d32-ad3b5373c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('리뷰_Sentence_Piece',axis=1)\n",
    "df = df.drop('리뷰_불용어제거',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddddf54-5bfb-4d33-a9c6-a519d5f683f2",
   "metadata": {},
   "source": [
    "### 공백단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad84bc7-0bf8-4c35-b7ac-79156f843221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백 단위 토큰화 함수 정의\n",
    "def split_tokenize(text):\n",
    "    if isinstance(text, str):  # 텍스트가 문자열인지 확인\n",
    "        return text.split()\n",
    "    else:\n",
    "        return []  # 텍스트가 아닌 경우 빈 리스트 반환\n",
    "\n",
    "# 공백 단위 토큰화 적용\n",
    "df['공백단위_토큰화'] = df['리뷰'].apply(split_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e92ffd-c442-433d-a9e6-88948f3deb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임의 토큰화된 리뷰 컬럼에 대해 불용어 제거 적용\n",
    "df['리뷰_불용어제거'] = df['공백단위_토큰화'].apply(remove_stopwords)\n",
    "\n",
    "# 결과 확인\n",
    "df[['공백단위_토큰화', '리뷰_불용어제거']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2464e0-8ef5-44ac-ae2c-3fc1cbcbbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('공백단위_토큰화',axis=1)\n",
    "df.to_csv('공백 토큰화.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4334d3-cfe5-4523-8008-bdfc3c78c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('리뷰_불용어제거',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04f2e5-964d-4191-a689-3f075a1e6cf7",
   "metadata": {},
   "source": [
    "### 4) 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028c73e-0b60-487a-ac39-565a0050b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774a4bd-f354-4a9d-b1c6-5c3c964a653c",
   "metadata": {},
   "source": [
    "### subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc076e-0429-4110-98ae-43acd4c46de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 로드 (예: 공백 토큰화된 파일 사용)\n",
    "df = pd.read_csv('/content/drive/MyDrive/BITAminNLP /CSV Files/subword 토큰화.csv')\n",
    "\n",
    "# 표제어 추출 함수 정의\n",
    "def lemmatize(text):\n",
    "    # 형태소 분석을 통해 표제어 추출\n",
    "    tokens = okt.pos(text, norm=True, stem=True)\n",
    "    return [word for word, tag in tokens]  # 표제어 리스트로 반환\n",
    "\n",
    "# 표제어 추출 적용\n",
    "df['리뷰_표제어추출'] = df['리뷰_불용어제거'].apply(lambda x: ' '.join(lemmatize(x)))\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['리뷰_불용어제거', '리뷰_표제어추출']].head())\n",
    "\n",
    "# 전처리된 파일 저장\n",
    "df.to_csv('Subword_표제어추출.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87a873-c59f-440e-ad13-178e781df59b",
   "metadata": {},
   "source": [
    "### 공백단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38698c23-9188-4a9f-b289-c91dd06a604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/공백 토큰화.csv')\n",
    "\n",
    "# 표제어 추출 적용\n",
    "df['리뷰_표제어추출'] = df['리뷰_불용어제거'].apply(lambda x: ' '.join(lemmatize(x)))\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['리뷰_불용어제거', '리뷰_표제어추출']].head())\n",
    "\n",
    "# 전처리된 파일 저장\n",
    "df.to_csv('공백토큰화_표제어추출.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2916b9-c768-4def-8d07-b907a4a085de",
   "metadata": {},
   "source": [
    "### 형태소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c5b9db-1cc6-48f7-86aa-7fd5fe64bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/BITAminNLP /CSV Files/형태소토큰화.csv')\n",
    "\n",
    "# 표제어 추출 적용\n",
    "df['리뷰_표제어추출'] = df['리뷰_불용어제거'].apply(lambda x: ' '.join(lemmatize(x)))\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['리뷰_불용어제거', '리뷰_표제어추출']].head())\n",
    "\n",
    "# 전처리된 파일 저장\n",
    "df.to_csv('형태소토큰화_표제어추출.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394f0ff-9f2a-423d-b531-e813fa2baeb4",
   "metadata": {},
   "source": [
    "## 2. TF-IDF 키워드 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7559c0d-b5af-4f55-85f3-ac6b2695651e",
   "metadata": {},
   "source": [
    "### 1) 관광지별 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b2e3d-6e5e-4b06-85da-16a7c521369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6acdb2-0a77-4180-a696-804489865378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2e614-c313-4151-9c45-c7512a4d041e",
   "metadata": {},
   "source": [
    "### subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129e8ca-d61c-47dd-a32b-70f2d927f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Subword_표제어추출.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a13647-8cf1-412c-991e-d69f75d5474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '전처리된_댓글' 열의 값을 문자열로 변환하고 결측치를 빈 문자열로 채움\n",
    "df['리뷰_표제어추출'] = df['리뷰_표제어추출'].fillna('').astype(str)\n",
    "\n",
    "# 사용자 지정 불용어 목록\n",
    "custom_stop_words = ['싶다', '좋다','가보다', '징쨔', '더욱','이다','보다','해보다','많다',]\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 (사용자 지정 불용어 포함)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "\n",
    "# 관광지별 댓글을 그룹화하여 하나의 문자열로 합침\n",
    "grouped = df.groupby('관광지')['리뷰_표제어추출'].apply(' '.join).reset_index()\n",
    "\n",
    "# TF-IDF 행렬 생성\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(grouped['리뷰_표제어추출'])\n",
    "\n",
    "# TF-IDF 점수 매트릭스를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=grouped['관광지'])\n",
    "\n",
    "# 관광지별 상위 5개 키워드 추출\n",
    "top_keywords = {}\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    top_keywords[idx] = row.nlargest(10).index.tolist()\n",
    "\n",
    "# 결과 출력\n",
    "for tourist_spot, keywords in top_keywords.items():\n",
    "    print(f'{tourist_spot}의 상위 10개 키워드: {\", \".join(keywords)}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(list(top_keywords.items()), columns=['관광지', '키워드'])\n",
    "\n",
    "# csv저장\n",
    "results_df.to_csv('TF-IDF subword 표제어추출.CSV', index=False, encoding='utf-8-sig')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d511343-cbe6-4824-85a8-bf06020494f9",
   "metadata": {},
   "source": [
    "### 공백단위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bde0f-8e3e-4d1c-b686-eb5b348ea6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('공백토큰화_표제어추출.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a807b0-0a77-4179-a2a0-0fed09bf2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '전처리된_댓글' 열의 값을 문자열로 변환하고 결측치를 빈 문자열로 채움\n",
    "df['리뷰_표제어추출'] = df['리뷰_표제어추출'].fillna('').astype(str)\n",
    "\n",
    "# 사용자 지정 불용어 목록\n",
    "custom_stop_words = ['싶다', '좋다','가보다', '징쨔', '더욱','이다','보다','해보다','많다',]\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 (사용자 지정 불용어 포함)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "\n",
    "# 관광지별 댓글을 그룹화하여 하나의 문자열로 합침\n",
    "grouped = df.groupby('관광지')['리뷰_표제어추출'].apply(' '.join).reset_index()\n",
    "\n",
    "# TF-IDF 행렬 생성\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(grouped['리뷰_표제어추출'])\n",
    "\n",
    "# TF-IDF 점수 매트릭스를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=grouped['관광지'])\n",
    "\n",
    "# 관광지별 상위 5개 키워드 추출\n",
    "top_keywords = {}\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    top_keywords[idx] = row.nlargest(10).index.tolist()\n",
    "\n",
    "# 결과 출력\n",
    "for tourist_spot, keywords in top_keywords.items():\n",
    "    print(f'{tourist_spot}의 상위 10개 키워드: {\", \".join(keywords)}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(list(top_keywords.items()), columns=['관광지', '키워드'])\n",
    "\n",
    "# csv저장\n",
    "results_df.to_csv('TF-IDF 공백 표제어 추출.CSV', index=False, encoding='utf-8-sig')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8dee2-7e98-4f38-acd8-389628c608f5",
   "metadata": {},
   "source": [
    "### 형태소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560e13a-c569-46c1-aa6b-416247a00076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('형태소토큰화_표제어추출.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140370a-ae96-4736-b12e-c46bcc50a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '전처리된_댓글' 열의 값을 문자열로 변환하고 결측치를 빈 문자열로 채움\n",
    "df['리뷰_표제어추출'] = df['리뷰_표제어추출'].fillna('').astype(str)\n",
    "\n",
    "# 사용자 지정 불용어 목록\n",
    "custom_stop_words = ['싶다', '좋다','가보다', '징쨔', '더욱','이다','보다','해보다','많다',]\n",
    "\n",
    "# TF-IDF 벡터라이저 초기화 (사용자 지정 불용어 포함)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stop_words)\n",
    "\n",
    "\n",
    "# 관광지별 댓글을 그룹화하여 하나의 문자열로 합침\n",
    "grouped = df.groupby('관광지')['리뷰_표제어추출'].apply(' '.join).reset_index()\n",
    "\n",
    "# TF-IDF 행렬 생성\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(grouped['리뷰_표제어추출'])\n",
    "\n",
    "# TF-IDF 점수 매트릭스를 데이터프레임으로 변환\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=grouped['관광지'])\n",
    "\n",
    "# 관광지별 상위 5개 키워드 추출\n",
    "top_keywords = {}\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    top_keywords[idx] = row.nlargest(10).index.tolist()\n",
    "\n",
    "# 결과 출력\n",
    "for tourist_spot, keywords in top_keywords.items():\n",
    "    print(f'{tourist_spot}의 상위 10개 키워드: {\", \".join(keywords)}')\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(list(top_keywords.items()), columns=['관광지', '키워드'])\n",
    "\n",
    "# csv저장\n",
    "results_df.to_csv('형태소토큰화_표제어추출.CSV', index=False, encoding='utf-8-sig')\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f06804-ffe4-440d-ae59-3ff387724e62",
   "metadata": {},
   "source": [
    "### 2) 키워드별 관광지 분류(키워드 10개)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05cb70-c2d0-4e59-9760-6d214d5c84e8",
   "metadata": {},
   "source": [
    "### 공백 표제어 추출 (최종 선택 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7f13e-4ebf-452a-85da-ba5bb02a8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/TF-IDF TEST.CSV'\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 불용어 리스트 파일 경로\n",
    "stopword_file_path = 'stopword.txt'\n",
    "\n",
    "# 텍스트 파일에서 불용어 리스트 불러오기\n",
    "with open(stopword_file_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "# 추가할 불용어 리스트\n",
    "additional_stopwords = {'하다', '있다', '너무', '대구', '에서', '오다', '올라가다'}\n",
    "stopwords.update(additional_stopwords)\n",
    "\n",
    "def clean_keywords(keywords):\n",
    "    # 문자열을 리스트로 변환하고 불용어 제거\n",
    "    keywords_list = ast.literal_eval(keywords)\n",
    "    return [word.lower() for word in keywords_list if word.lower() not in stopwords]\n",
    "\n",
    "# 키워드를 리스트로 변환하고 불용어 제거\n",
    "data['키워드'] = data['키워드'].apply(clean_keywords)\n",
    "\n",
    "# 유의어 처리 및 '군위' 제외\n",
    "synonyms = {\n",
    "    '공원': '산책','사진': '전망대',\n",
    "    '놀다' : '체험', '먹다': '시장'\n",
    "}\n",
    "\n",
    "def replace_synonyms(keywords):\n",
    "    return [synonyms.get(word, word) for word in keywords if word.lower() != '군위']\n",
    "\n",
    "data['키워드'] = data['키워드'].apply(replace_synonyms)\n",
    "\n",
    "# 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords = [keyword for sublist in data['키워드'] for keyword in sublist]\n",
    "\n",
    "# 키워드 빈도 계산\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# 상위 N개의 키워드 추출\n",
    "top_n = 10\n",
    "top_keywords = [keyword for keyword, _ in keyword_counts.most_common(top_n)]\n",
    "\n",
    "# 상위 N개 키워드에 해당하는 관광지 리스트 매핑\n",
    "keyword_to_places = {keyword: [] for keyword in top_keywords}\n",
    "\n",
    "for keyword in top_keywords:\n",
    "    places = data.loc[data['키워드'].apply(lambda x: keyword in x), '관광지 이름'].unique().tolist()\n",
    "    keyword_to_places[keyword] = sorted(places)  # 관광지 리스트를 정렬하여 저장\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "result_df = pd.DataFrame([(keyword, ', '.join(places)) for keyword, places in keyword_to_places.items()],\n",
    "                         columns=['키워드', '관광지 리스트'])\n",
    "\n",
    "# 결과 저장\n",
    "result_file_path = '공백표제어_관광지리스트.csv'\n",
    "result_df.to_csv(result_file_path, index=False, encoding='utf-8-sig')  # UTF-8 BOM 추가\n",
    "\n",
    "# 결과 파일 경로 출력\n",
    "print(\"결과 파일 경로:\", result_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad73d30-f154-404b-9307-341cfab1ac81",
   "metadata": {},
   "source": [
    "### 형태소 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27777418-9ee9-4513-bd69-8e5f8ae10479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "file_path = './data/형태소토큰화_표제어추출.CSV'\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 불용어 리스트 파일 경로\n",
    "stopword_file_path = './data/stopword.txt'\n",
    "\n",
    "# 텍스트 파일에서 불용어 리스트 불러오기\n",
    "with open(stopword_file_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 추가할 불용어 리스트\n",
    "additional_stopwords = ['하다', '있다', '너무', '대구']\n",
    "\n",
    "# 기존 불용어 리스트에 추가 불용어를 합치기\n",
    "stopwords.extend(additional_stopwords)\n",
    "\n",
    "# 키워드를 리스트로 변환하고 불용어 제거\n",
    "data['키워드'] = data['키워드'].apply(lambda x: [word for word in ast.literal_eval(x) if word not in stopwords])\n",
    "\n",
    "# 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords = [keyword for sublist in data['키워드'] for keyword in sublist]\n",
    "\n",
    "# 키워드 빈도 계산\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# 가장 많이 언급된 상위 10개 키워드 추출\n",
    "top_keywords = [keyword for keyword, _ in keyword_counts.most_common(10)]\n",
    "\n",
    "# 상위 10개 키워드에 해당하는 관광지 리스트 매핑\n",
    "keyword_to_places = {keyword: [] for keyword in top_keywords}\n",
    "\n",
    "for keyword in top_keywords:\n",
    "    places = data.loc[data['키워드'].apply(lambda x: keyword in x), '관광지명'].unique().tolist()\n",
    "    keyword_to_places[keyword] = places\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "result_df = pd.DataFrame([(keyword, ', '.join(places)) for keyword, places in keyword_to_places.items()], \n",
    "                         columns=['키워드', '관광지 리스트'])\n",
    "\n",
    "# 결과 저장\n",
    "result_file_path = './data/형태소토큰화_관광지리스트.csv'\n",
    "result_df.to_csv(result_file_path, index=False)\n",
    "\n",
    "# 결과 파일 경로 출력\n",
    "result_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6f496-e95e-4116-b567-c5fbb73dde72",
   "metadata": {},
   "source": [
    "### 서브워드 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266966eb-3057-45e9-9ac7-7d19146bca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로\n",
    "file_path = './data/TF-IDF subword 표제어추출.CSV'\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 불용어 리스트 파일 경로\n",
    "stopword_file_path = './data/stopword.txt'\n",
    "\n",
    "# 텍스트 파일에서 불용어 리스트 불러오기\n",
    "with open(stopword_file_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# 추가할 불용어 리스트\n",
    "additional_stopwords = ['하다', '있다', '너무', '대구']\n",
    "\n",
    "# 기존 불용어 리스트에 추가 불용어를 합치기\n",
    "stopwords.extend(additional_stopwords)\n",
    "\n",
    "# 키워드를 리스트로 변환하고 불용어 제거\n",
    "data['키워드'] = data['키워드'].apply(lambda x: [word for word in ast.literal_eval(x) if word not in stopwords])\n",
    "\n",
    "# 모든 키워드를 하나의 리스트로 합치기\n",
    "all_keywords = [keyword for sublist in data['키워드'] for keyword in sublist]\n",
    "\n",
    "# 키워드 빈도 계산\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# 가장 많이 언급된 상위 10개 키워드 추출\n",
    "top_keywords = [keyword for keyword, _ in keyword_counts.most_common(10)]\n",
    "\n",
    "# 상위 10개 키워드에 해당하는 관광지 리스트 매핑\n",
    "keyword_to_places = {keyword: [] for keyword in top_keywords}\n",
    "\n",
    "for keyword in top_keywords:\n",
    "    places = data.loc[data['키워드'].apply(lambda x: keyword in x), '관광지'].unique().tolist()\n",
    "    keyword_to_places[keyword] = places\n",
    "\n",
    "# 결과를 데이터프레임으로 변환\n",
    "result_df = pd.DataFrame([(keyword, ', '.join(places)) for keyword, places in keyword_to_places.items()], \n",
    "                         columns=['키워드', '관광지 리스트'])\n",
    "\n",
    "# 결과 저장\n",
    "result_file_path = './data/서브워드토큰화_관광지리스트.csv'\n",
    "result_df.to_csv(result_file_path, index=False)\n",
    "\n",
    "# 결과 파일 경로 출력\n",
    "result_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f997a-6c15-43c4-9842-c9dc2bba0395",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. 감정분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41504003-b8f8-41d1-9209-c4ecba8aee1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 리뷰_표제어추출\n",
    "\n",
    "1: 긍정적인 감정\n",
    "0: 부정적인 감정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd6559-6b0a-4b79-ab5f-f3b6a8b1fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# 데이터 파일 로드 (파일 경로를 정확히 지정)\n",
    "file_path = '/content/형태소_sampled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 사전 학습된 모델과 토크나이저 로드\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# 감정 예측 함수 정의\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # 배치 처리 중에는 필요하지 않은 계산 생략\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 모델을 CPU 또는 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 표제어추출 데이터셋 준비 및 감정 분석 (최적화 적용)\n",
    "lemma_reviews = data['리뷰_표제어추출'].dropna().tolist()\n",
    "lemma_dataset = ReviewDataset(lemma_reviews, tokenizer, max_length=128)\n",
    "lemma_dataloader = DataLoader(lemma_dataset, batch_size=32, shuffle=False, num_workers=4)  # 배치 크기 증가, 멀티프로세싱 사용\n",
    "\n",
    "lemma_predictions = predict_sentiment(model, lemma_dataloader)\n",
    "\n",
    "# 예측 결과를 원래 데이터프레임에 추가\n",
    "data.loc[data['리뷰_표제어추출'].notna(), 'Sentiment_Lemma'] = lemma_predictions\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv('sentiment_analysis_lemma.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and results saved to 'sentiment_analysis_lemma.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331edfc-b233-4f17-bc7e-6def4c91c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 첫 5행 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd02169-2341-41f6-9fa3-3c39eae274ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 관광지 이름 별로 Sentiment 0과 1의 개수를 세는 코드\n",
    "sentiment_counts = data.groupby('관광지')['Sentiment_Lemma'].value_counts().unstack().fillna(0)\n",
    "\n",
    "\n",
    "# 감정 분포 확인\n",
    "sentiment_distribution = data['Sentiment_Lemma'].value_counts()\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# 결과 확인\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ae6c2-b1f4-4568-8f7c-22e10a985456",
   "metadata": {},
   "source": [
    "### 리뷰_불용어제거\n",
    "1: 긍정적인 감정 0: 부정적인 감정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95429edc-9099-4f44-85bc-0b2e82ac9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# 데이터 파일 로드 (파일 경로를 정확히 지정)\n",
    "file_path = '/content/형태소_sampled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 사전 학습된 모델과 토크나이저 로드\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# 감정 예측 함수 정의\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # 배치 처리 중에는 필요하지 않은 계산 생략\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 모델을 CPU 또는 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 표제어추출 데이터셋 준비 및 감정 분석 (최적화 적용)\n",
    "lemma_reviews = data['리뷰_불용어제거'].dropna().tolist()\n",
    "lemma_dataset = ReviewDataset(lemma_reviews, tokenizer, max_length=128)\n",
    "lemma_dataloader = DataLoader(lemma_dataset, batch_size=32, shuffle=False, num_workers=4)  # 배치 크기 증가, 멀티프로세싱 사용\n",
    "\n",
    "lemma_predictions = predict_sentiment(model, lemma_dataloader)\n",
    "\n",
    "# 예측 결과를 원래 데이터프레임에 추가\n",
    "data.loc[data['리뷰_불용어제거'].notna(), 'Sentiment_Lemma'] = lemma_predictions\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv('sentiment_analysis_lemma_2.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and results saved to 'sentiment_analysis_lemma.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8be38-a7cf-42dc-bcc4-f00ddf8166a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma_2.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 첫 5행 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63714a54-690e-44b5-8618-4ff3083e882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma_2.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 관광지 이름 별로 Sentiment 0과 1의 개수를 세는 코드\n",
    "sentiment_counts = data.groupby('관광지')['Sentiment_Lemma'].value_counts().unstack().fillna(0)\n",
    "\n",
    "\n",
    "# 감정 분포 확인\n",
    "sentiment_distribution = data['Sentiment_Lemma'].value_counts()\n",
    "print(sentiment_distribution)\n",
    "\n",
    "# 결과 확인\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67c35b-1f14-459b-a662-89b96713bf74",
   "metadata": {},
   "source": [
    "### 리뷰 표제어 추출값으로 확률 값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38689508-b1b7-4079-82e5-83b7f6627e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F  # Softmax를 위해 필요\n",
    "\n",
    "# 데이터 파일 로드 (파일 경로를 정확히 지정)\n",
    "file_path = '/content/형태소_sampled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 사전 학습된 모델과 토크나이저 로드\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "# 감정 예측 함수 정의 (확률로 반환)\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # 소프트맥스를 사용해 확률로 변환\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            positive_probs = probs[:, 1].cpu().numpy()  # 긍정 클래스(1)의 확률을 가져옴\n",
    "            predictions.extend(positive_probs)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 모델을 CPU 또는 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 표제어추출 데이터셋 준비 및 감정 분석 (최적화 적용)\n",
    "lemma_reviews = data['리뷰_표제어추출'].dropna().tolist()\n",
    "lemma_dataset = ReviewDataset(lemma_reviews, tokenizer, max_length=128)\n",
    "lemma_dataloader = DataLoader(lemma_dataset, batch_size=32, shuffle=False, num_workers=4)  # 배치 크기 증가, 멀티프로세싱 사용\n",
    "\n",
    "lemma_predictions = predict_sentiment(model, lemma_dataloader)\n",
    "\n",
    "# 예측 결과를 원래 데이터프레임에 추가\n",
    "data.loc[data['리뷰_표제어추출'].notna(), 'Sentiment_Lemma'] = lemma_predictions\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv('sentiment_analysis_lemma_percent.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and results saved to 'sentiment_analysis_lemma.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2583f6-a09a-40b3-81b5-cd9311f0697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma_percent.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 첫 5행 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949e3c3-efce-4871-b7f2-9f7be4c43ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 파일 로드\n",
    "file_path = '/content/sentiment_analysis_lemma_percent.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Sentiment_Lemma 확률 값 추출\n",
    "sentiment_probs = data['Sentiment_Lemma'].dropna()  # NaN 값 제거\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentiment_probs, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment_Lemma Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593963a8-9753-44a5-b970-6321c7794cf3",
   "metadata": {},
   "source": [
    "### 모델 고도화\n",
    "\n",
    "BERT 모델 바꿔보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d63884-68e7-4a70-82c3-d9ca75da7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 데이터 파일 로드\n",
    "file_path = '/content/형태소_sampled.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            positive_probs = probs[:, 1].cpu().numpy()\n",
    "            predictions.extend(positive_probs)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "lemma_reviews = data['리뷰_표제어추출'].dropna().tolist()\n",
    "lemma_dataset = ReviewDataset(lemma_reviews, tokenizer, max_length=128)\n",
    "lemma_dataloader = DataLoader(lemma_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "lemma_predictions = predict_sentiment(model, lemma_dataloader)\n",
    "\n",
    "data.loc[data['리뷰_표제어추출'].notna(), 'Sentiment_Lemma'] = lemma_predictions\n",
    "data.to_csv('sentiment_analysis_lemma_percent.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed and results saved to 'sentiment_analysis_lemma_percent.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9570ea-448b-4f44-bad4-43475a3d53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma_percent.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 첫 5행 확인\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3b8d2-00f9-4261-adbd-6b0c24375da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일 로드\n",
    "file_path = '/content/sentiment_analysis_lemma_percent.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Sentiment_Lemma 확률 값 추출\n",
    "sentiment_probs = data['Sentiment_Lemma'].dropna()  # NaN 값 제거\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentiment_probs, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment_Lemma Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae856b89-eea0-4b51-a22f-508b1bddbdec",
   "metadata": {},
   "source": [
    "### 모델 앙상블 (최종 선택 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b768d7e-2600-44f0-8cc7-5ac4e114711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 데이터 파일 로드\n",
    "file_path = '/content/형태소.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 두 개의 BERT 모델 로드\n",
    "model1 = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model2 = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 모델을 CPU 또는 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "        }\n",
    "\n",
    "def predict_sentiment(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            positive_probs = probs[:, 1].cpu().numpy()\n",
    "            predictions.extend(positive_probs)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 데이터셋 준비\n",
    "lemma_reviews = data['리뷰_불용어제거'].dropna().tolist()\n",
    "lemma_dataset = ReviewDataset(lemma_reviews, tokenizer, max_length=128)\n",
    "lemma_dataloader = DataLoader(lemma_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 모델 1 예측\n",
    "print(\"Predicting with Model 1...\")\n",
    "lemma_predictions_model1 = predict_sentiment(model1, lemma_dataloader)\n",
    "\n",
    "# 모델 2 예측\n",
    "print(\"Predicting with Model 2...\")\n",
    "lemma_predictions_model2 = predict_sentiment(model2, lemma_dataloader)\n",
    "\n",
    "# 두 모델의 예측을 평균화\n",
    "ensemble_predictions = [(p1 + p2) / 2 for p1, p2 in zip(lemma_predictions_model1, lemma_predictions_model2)]\n",
    "\n",
    "# 예측 결과를 원래 데이터프레임에 추가\n",
    "data.loc[data['리뷰_불용어제거'].notna(), 'Sentiment_Lemma'] = ensemble_predictions\n",
    "\n",
    "# 결과 저장\n",
    "data.to_csv('sentiment_analysis_lemma_ensemble_bert.csv', index=False)\n",
    "\n",
    "print(\"Sentiment analysis completed with ensemble model and results saved to 'sentiment_analysis_lemma_ensemble.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd2ed7-16ed-4fe8-972c-26396267ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 긴 텍스트가 잘리지 않도록 설정\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = '/content/sentiment_analysis_lemma_ensemble_bert.csv'\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 로드\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임의 첫 5행 확인\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779dc7a8-31c5-4093-a4bc-dd1f01555675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 파일 로드\n",
    "file_path = '/content/sentiment_analysis_lemma_ensemble_bert.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Sentiment_Lemma 확률 값 추출\n",
    "sentiment_probs = data['Sentiment_Lemma'].dropna()  # NaN 값 제거\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentiment_probs, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Sentiment_Lemma Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
